Codes included in this repository were part of the poster presentation
which took place on the 22nd annual meeting of the Association for the 
Scientific Study of Consciousness (ASSC).

The central idea of this onoging work starts from the observation that
neural networks are universal function approximators; namely, NNs have 
the potential to represent a wide spectrum of functions. However, a 
typical training procedure will make a NN converge to _one_ function. 
I added an additional level of flexibility by allowing connectivities 
in a network to be switched on or off. The switching mechanism is 
derived from prediction error that drives a secondary generative model, 
from which dropout pattern can be sampled and applied to the primary
network. As a result, this new type of neural network can adapt quickly
to predict nonstationary signals. 

Youtube: [https://youtu.be/l4mmD776Ehw](https://youtu.be/l4mmD776Ehw)
